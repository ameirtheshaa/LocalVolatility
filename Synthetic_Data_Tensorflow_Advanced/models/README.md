# Models Directory

This directory contains trained neural network models and output from the pipeline.

## Directory Structure

```
models/
├── example_pretrained/            # Pre-trained example models
│   ├── NN_phi_final.keras         # Trained option price network
│   └── NN_eta_final.keras         # Trained local volatility network
│
└── runs/                          # Experiment outputs (auto-organized)
    └── <experiment_name>/         # Timestamped or custom directory
        ├── NN_phi_final.keras
        ├── NN_eta_final.keras
        ├── metadata.json          # Configuration and scaling parameters
        ├── training_data.npz      # Monte Carlo training data
        ├── losses_*.png           # Training progress plots
        └── pdf_analysis_*.png     # PDF validation plots
```

## Pre-trained Example Models

The `example_pretrained/` directory contains models trained with:

- **Volatility model**: Dupire exact (synthetic)
- **Architecture**: 3 residual blocks, 64 units
- **Training**: 30,000 epochs
- **Data**: 10,000 Monte Carlo paths

These models can be used for:
- Testing the analysis pipeline
- Learning how to use the code
- Comparing your results

### Using Pre-trained Models

```python
from config import DupirePipelineConfig
from dupire_pipeline import DupirePipeline

# Load and analyze pre-trained models
config = DupirePipelineConfig.analysis_only('models/example_pretrained')
pipeline = DupirePipeline(config)
pipeline.run()
```

Or via command line:

```bash
python examples/run_analysis_only.py --model-dir models/example_pretrained
```

## Training Your Own Models

When you run training, output is saved to a timestamped directory:

```bash
python examples/run_full_training.py
# Creates: models/runs/synthetic_data_3resblock_20250129_143022/
```

You can specify a custom directory:

```bash
python dupire_pipeline.py --output-dir models/runs/my_experiment
```

## Model Files

### NN_phi_final.keras
- **Purpose**: Neural network for option prices
- **Input**: (t_tilde, k_tilde) - scaled time and strike
- **Output**: φ_tilde - scaled option price
- **Size**: ~200 KB
- **Format**: Keras SavedModel

### NN_eta_final.keras
- **Purpose**: Neural network for local volatility squared
- **Input**: (t_tilde, k_tilde)
- **Output**: η_tilde - scaled local volatility squared
- **Size**: ~200 KB
- **Format**: Keras SavedModel

### metadata.json
- Configuration used for training
- Scaling parameters (S₀, r, T_max, K_max)
- Volatility model parameters
- Training statistics

**Important**: When loading models for analysis, the scaling parameters must match those used during training. This information is stored in metadata.json.

### training_data.npz
- Training data generated by Monte Carlo simulation
- Contains: (T, K, φ) tuples
- Can be reused for training with different hyperparameters
- Avoid regenerating if you're just tuning the network

## Loading Models in Python

```python
import tensorflow as tf

# Load models directly
nn_phi = tf.keras.models.load_model('path/to/NN_phi_final.keras')
nn_eta = tf.keras.models.load_model('path/to/NN_eta_final.keras')

# Query option price
import numpy as np
t_tilde = np.array([[0.5]])  # t = 0.75 years if T_max = 1.5
k_tilde = np.array([[0.7]])  # Normalized strike
phi_tilde = nn_phi.predict([t_tilde, k_tilde])

# Query local volatility
eta_tilde = nn_eta.predict([t_tilde, k_tilde])
sigma = np.sqrt(2 * eta_tilde / T_max)
```

## Storage Requirements

Typical storage per experiment:

- Models: ~400 KB (NN_phi + NN_eta)
- Training data: ~1-10 MB (depends on N_strikes × N_maturities)
- Plots: ~1-5 MB (depends on DPI and number of plots)
- **Total**: ~5-20 MB per experiment

For 10 experiments: ~50-200 MB

## Cleaning Up

To remove old experiments:

```bash
# List all model directories
ls -lh models/runs/

# Remove a specific experiment
rm -rf models/runs/synthetic_data_3resblock_20250101_120000

# Keep only the latest 5 experiments
ls -t models/runs/ | tail -n +6 | xargs -I {} rm -rf models/runs/{}
```

## Best Practices

1. **Use descriptive names**:
   ```bash
   python dupire_pipeline.py --output-dir models/runs/smile_surface_lambda2
   ```

2. **Keep metadata.json**:
   - Essential for reproducing results
   - Needed for correct scaling when loading models

3. **Archive successful experiments**:
   ```bash
   tar -czf experiment_2025-01-29.tar.gz models/runs/synthetic_data_3resblock_20250129_143022/
   ```

4. **Document hyperparameters**:
   - metadata.json captures most settings automatically
   - Add a README.txt in your experiment directory for notes

## Troubleshooting

**Problem**: Model files not found

```
Solution: Ensure files are named exactly:
  - NN_phi_final.keras (or NN_phi.keras)
  - NN_eta_final.keras (or NN_eta.keras)
```

**Problem**: Scaling mismatch when loading models

```
Solution: Load metadata.json to get correct scaling parameters:
  import json
  with open('models/runs/my_exp/metadata.json') as f:
      metadata = json.load(f)
  S0 = metadata['scaling']['S0']
  r = metadata['scaling']['r']
```

**Problem**: Out of disk space

```
Solution:
  1. Delete old experiments
  2. Compress old experiments: tar -czf old.tar.gz models/runs/old_exp/
  3. Disable plot saving: config.plot_config.enable_training_plots = False
```

---

For more information, see the main [README.md](../README.md) in the parent directory.
